{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from text_cleaning import text_process_nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FOLDER = \"../Data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"liste_des_reponses_aux_questions_ouvertes_par_consultation_2023-11-02T10_21_31.386925747Z.csv\"\n",
    "filename_with_demo = \"reponses_aux_questions_ouvertes_croisees_avec_les_donnees_demo_2023-11-02T13_09_36.480435917Z.csv\"\n",
    "df = pd.read_csv(DATA_FOLDER + filename)\n",
    "df_demo = pd.read_csv(DATA_FOLDER + filename_with_demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_COL = \"Questions → Title\"\n",
    "RESPONSE_COL = \"Response Text\"\n",
    "TOKEN_COL = \"tokens\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Questions existantes\n",
    "df[QUESTION_COL].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Remove, useless answers\n",
    "def clean_df(df: pd.DataFrame, size: int=10):\n",
    "    cleaned_df = df.copy()\n",
    "    cleaned_df[\"response_size\"] = cleaned_df[RESPONSE_COL].str.split().apply(len)\n",
    "    cleaned_df = cleaned_df.drop(cleaned_df[cleaned_df[\"response_size\"] < size].index)\n",
    "    WHITESPACE_HANDLER = lambda k: re.sub('\\\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "    cleaned_df[\"cleaned_text\"] = cleaned_df[\"Response Text\"].apply(WHITESPACE_HANDLER)\n",
    "    return cleaned_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered = df[df[QUESTION_COL] == \"Quelles sont vos autres propositions pour lutter contre les violences faites aux enfants ?\"].copy()\n",
    "question = 'Avez-vous des propositions pour financer la transition écologique ? C’est la dernière question !'\n",
    "df_filtered = df[df[QUESTION_COL] == question].copy()\n",
    "df_filtered = clean_df(df_filtered)\n",
    "df_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = df_filtered[RESPONSE_COL]\n",
    "X = df_filtered[\"cleaned_text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfid = TfidfVectorizer(analyzer=text_process_nltk, ngram_range=(1,3)).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered[TOKEN_COL] = df_filtered[RESPONSE_COL].apply(text_process_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_process_nltk(\"Légiférer au sujet de l'aliénation parentale\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def get_token_test(doc: str):\n",
    "    for i in doc:\n",
    "        print(i)\n",
    "        print(word_tokenize(i, language='french'))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : Enlever les \"l'\" des mots avant d'enlever le '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_topic = df_filtered[TOKEN_COL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "import gensim\n",
    "import pickle\n",
    "\n",
    "\n",
    "dictionary = corpora.Dictionary(X_topic)\n",
    "corpus = [dictionary.doc2bow(text) for text in X_topic]\n",
    "\n",
    "#pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "#dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "NUM_TOPICS = 4\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary, passes=15)\n",
    "#ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary.load('dictionary.gensim')\n",
    "corpus = pickle.load(open('corpus.pkl', 'rb'))\n",
    "lda = gensim.models.ldamodel.LdaModel.load('model5.gensim')\n",
    "import pyLDAvis.gensim\n",
    "lda_display = pyLDAvis.gensim.prepare(lda, corpus, dictionary, sort_topics=False)\n",
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we assume that the articles in the same topic contain roughly the same information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Histogramme, des mots par topic\n",
    "# Essayer sans passer par résumé de chaque échantillon\n",
    "# Créer 5 titres et agréger pour réduire le bruit\n",
    "# Check repetition_penalty param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_topic.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# download dataset of 20,000 news articles\n",
    "#docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "docs = df_filtered[RESPONSE_COL].values\n",
    "\n",
    "sentence_model = SentenceTransformer(\"dangvantuan/sentence-camembert-base\")\n",
    "embeddings = sentence_model.encode(docs)\n",
    "\n",
    "# Train BERTopic with a custom CountVectorizer\n",
    "vectorizer_model = CountVectorizer(strip_accents=\"ascii\")\n",
    "#topic_model = BERTopic(vectorizer_model=vectorizer_model, embedding_model=sentence_model, nr_topics=6, language=\"french\")\n",
    "\n",
    "min_topic_size = 10 # Default=10\n",
    "\n",
    "topic_model = BERTopic(min_topic_size=min_topic_size, nr_topics=8, language=\"french\")\n",
    "topics = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "\n",
    "\n",
    "def get_custom_bertopic_model(X):\n",
    "    # Remove stopwords\n",
    "    #representation_model = MaximalMarginalRelevance(diversity=0.2)\n",
    "    vectorizer_model = CountVectorizer(stop_words=stopwords.words(\"french\"), strip_accents=\"ascii\")\n",
    "    topic_model = BERTopic(vectorizer_model=vectorizer_model, nr_topics=10, language=\"french\")\n",
    "    \n",
    "    # Reduce frequent word importance\n",
    "    #ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "    #topic_model = BERTopic(ctfidf_model=ctfidf_model, language=\"french\")\n",
    "    \n",
    "    topics = topic_model.fit_transform(X)\n",
    "    return topic_model, topics\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bert, custom_topics = get_custom_bertopic_model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom_bert.visualize_heatmap()\n",
    "custom_bert.visualize_barchart()\n",
    "#custom_bert.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def create_wordcloud(model, topic):\n",
    "    text = {word: value for word, value in model.get_topic(topic)}\n",
    "    wc = WordCloud(background_color=\"white\", max_words=1000)\n",
    "    wc.generate_from_frequencies(text)\n",
    "    plt.imshow(wc, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "# Show wordcloud\n",
    "create_wordcloud(custom_bert, topic=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic_list = list(range(8))\n",
    "#topic_model.visualize_approximate_distribution()\n",
    "# TODO: Score d'incertitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_infos = custom_bert.get_document_info(X)\n",
    "#docs_with_topics[docs_with_topics[\"Topic\"] == 2]\n",
    "doc_infos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_infos.Topic.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_distribution(doc_infos: pd.DataFrame):\n",
    "    answers_per_topic = doc_infos.groupby(\"Topic\").agg(answers=(\"Document\", \"count\")).reset_index()\n",
    "    answers_per_topic[\"percentage\"] = answers_per_topic[\"answers\"] / answers_per_topic[\"answers\"].values.sum() * 100\n",
    "    return answers_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_per_topic = get_topic_distribution(doc_infos)\n",
    "answers_per_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_docs_from_topic(doc_infos, topic):\n",
    "    representatives = doc_infos[doc_infos[\"Topic\"] == topic].copy()\n",
    "    #docs_with_topics[\"Representative_document\"]\n",
    "    return representatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization functions\n",
    "def get_topic_histogram(doc_infos, topic):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import TFT5ForConditionalGeneration, T5Tokenizer, pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from random import sample\n",
    "\n",
    "\n",
    "def get_tokenizer(t5=True):\n",
    "    if t5:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "    else: \n",
    "        #tokenizer = AutoTokenizer.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"moussaKam/barthez\")\n",
    "        #tokenizer = AutoTokenizer.from_pretrained(\"moussaKam/barthez-orangesum-title\")\n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def get_summarizer_pipeline(tokenizer, t5=True):\n",
    "    if t5:\n",
    "        language_model = TFT5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
    "        summarizer = pipeline(\"summarization\", model=language_model, tokenizer=tokenizer, framework=\"tf\")\n",
    "    else:\n",
    "        #summarizer = pipeline(\"summarization\", model=\"csebuetnlp/mT5_multilingual_XLSum\", tokenizer=tokenizer)\n",
    "        #summarizer = pipeline(\"summarization\", model=\"moussaKam/barthez\", tokenizer=tokenizer)\n",
    "        summarizer = pipeline(\"summarization\", model=\"moussaKam/barthez-orangesum-abstract\", tokenizer=tokenizer)\n",
    "    return summarizer\n",
    "\n",
    "\n",
    "def get_headline_generator(t5=True, model_name=\"\"):\n",
    "    if t5: \n",
    "        headline_generator = TFT5ForConditionalGeneration.from_pretrained(\"Michau/t5-base-en-generate-headline\")\n",
    "    else:\n",
    "        #headline_generator = AutoModelForSeq2SeqLM.from_pretrained(\"csebuetnlp/mT5_multilingual_XLSum\")\n",
    "        headline_generator = AutoModelForSeq2SeqLM.from_pretrained(\"moussaKam/barthez-orangesum-title\")\n",
    "    return headline_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_list_from_answers(answers: list[str], summarizer, tokenizer):\n",
    "    summary_list = []\n",
    "    current_token_length = 0\n",
    "    max_token_length = 500 # anciennement 512\n",
    "    #WHITESPACE_HANDLER = lambda k: re.sub('\\\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "    for answer in sample(answers, k=len(answers)):\n",
    "        max_length = min(150, max(10, len(tokenizer.encode(answer))))\n",
    "        summary = summarizer(answer, min_length=10, max_length=max_length)[0][\"summary_text\"]\n",
    "        current_token_length += len(tokenizer.encode(summary))\n",
    "        if current_token_length >= max_token_length:\n",
    "            break\n",
    "        summary_list.append(summary)\n",
    "    return summary_list\n",
    "\n",
    "\n",
    "def get_summary_list_from_answers_with_answer_sum(answers: list[str], summarizer, tokenizer):\n",
    "    summary_list = []\n",
    "    current_token_length = 0\n",
    "    max_token_length = 500 # anciennement 512\n",
    "    min_token_length = 100\n",
    "    to_summarize = \"\"\n",
    "    #WHITESPACE_HANDLER = lambda k: re.sub('\\\\s+', ' ', re.sub('\\n+', ' ', k.strip()))\n",
    "    for answer in sample(answers, k=len(answers)):\n",
    "        to_summarize = to_summarize + answer + \", \"\n",
    "        answer_size = len(tokenizer.encode(to_summarize))\n",
    "        print(str(answer_size) + \" \" + to_summarize)\n",
    "        if answer_size <= min_token_length: \n",
    "            continue\n",
    "        max_length = min(150, max(10, int(answer_size/2)))\n",
    "        to_summarize = to_summarize.replace(\".,\", \",\")\n",
    "        summary = summarizer(to_summarize, min_length=10, max_length=max_length)[0][\"summary_text\"]\n",
    "        print(\"to_summarize: \", to_summarize)\n",
    "        print(\"summary: \", summary)\n",
    "        to_summarize = \"\"\n",
    "        current_token_length += len(tokenizer.encode(summary))\n",
    "        if current_token_length >= max_token_length:\n",
    "            break\n",
    "        summary_list.append(summary)\n",
    "    return summary_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def generate_topic_label(answers: list[str], summarizer, tokenizer, headline_generator, verbose=False) -> str:\n",
    "    summary_list = get_summary_list_from_answers_with_answer_sum(answers, summarizer, tokenizer)\n",
    "    print(summary_list)\n",
    "    encoding = tokenizer.encode(\"Titre : \" + \" \".join(summary_list), return_tensors=\"pt\")\n",
    "    output = headline_generator.generate(encoding, max_length=64)\n",
    "    return tokenizer.decode(output[0][1:-1])\n",
    "\n",
    "\n",
    "def get_labels_from_topics(doc_infos, i_range, verbose=False, t5=True):\n",
    "    tokenizer = get_tokenizer(t5)\n",
    "    summarizer = get_summarizer_pipeline(tokenizer, t5)\n",
    "    headline_generator = get_headline_generator(t5)\n",
    "    topic_labels = []\n",
    "    for i in range(i_range):\n",
    "        topic_i = get_docs_from_topic(doc_infos, i)\n",
    "        doc_i = topic_i['Document'].values.tolist()\n",
    "        label = \"Topic \" + str(i) + \" : \" + generate_topic_label(doc_i, summarizer, tokenizer, headline_generator, verbose)\n",
    "        print(label)\n",
    "        topic_labels.append(label)\n",
    "    return topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_summary(question, summary_list):\n",
    "    result = \"Question : \" + question + \". Réponse : \" + \" \".join(summary_list)\n",
    "    return result\n",
    "\n",
    "\n",
    "def generate_topic_summary(answers: list[str], summarizer, tokenizer, question):\n",
    "    summary_list = get_summary_list_from_answers_with_answer_sum(answers, summarizer, tokenizer)\n",
    "    #formated_summary = format_summary(question, summary_list)\n",
    "    formated_summary = \" \".join(summary_list)\n",
    "    print(formated_summary)\n",
    "    return summarizer(formated_summary, max_length=150)[0][\"summary_text\"]\n",
    "\n",
    "\n",
    "def get_summary_from_topics(doc_infos, i_range, question, t5):\n",
    "    tokenizer = get_tokenizer(t5)\n",
    "    summarizer = get_summarizer_pipeline(tokenizer, t5)\n",
    "    topic_labels = []\n",
    "    for i in range(i_range):\n",
    "        topic_i = get_docs_from_topic(doc_infos, i)\n",
    "        doc_i = topic_i['Document'].values.tolist()\n",
    "        topic_summary = generate_topic_summary(doc_i, summarizer, tokenizer, question)\n",
    "        print(topic_summary)\n",
    "        label = \"Résumé \" + str(i) + \" : \" + topic_summary\n",
    "        print(label)\n",
    "        topic_labels.append(label)\n",
    "    return topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_range = sum(answers_per_topic[\"percentage\"] > 2) -1\n",
    "i_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5 = True\n",
    "topic_labels = get_labels_from_topics(doc_infos, i_range=i_range, verbose=True, t5=t5)\n",
    "for topic in topic_labels:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries = get_summary_from_topics(doc_infos, i_range=i_range, question=question, t5=t5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_n = get_docs_from_topic(doc_infos, 3)\n",
    "looking_for = \"solaire\"\n",
    "doc_n = topic_n[topic_n['Document'].str.contains(looking_for)][\"Document\"].values\n",
    "doc_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Topic 4 label: {generate_topic_label(doc_n.tolist())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KeyBert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "# Prepare documents \n",
    "docs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n",
    "\n",
    "# Extract keywords\n",
    "kw_model = KeyBERT()\n",
    "keywords = kw_model.extract_keywords(docs)\n",
    "\n",
    "# Create our vocabulary\n",
    "vocabulary = [k[0] for keyword in keywords for k in keyword]\n",
    "vocabulary = list(set(vocabulary))\n",
    "# Then, we pass our vocabulary to BERTopic and train the model:\n",
    "\n",
    "from bertopic import BERTopic\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer_model= CountVectorizer(vocabulary=vocabulary)\n",
    "topic_model = BERTopic(vectorizer_model=vectorizer_model)\n",
    "topics, probs = topic_model.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.generate_topic_labels(nr_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agora",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
